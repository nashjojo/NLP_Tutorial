%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%
%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}
\chapter{NLP发展现状与应用领域}
\label{basic} % Always give a unique label
% use \chaptermark{}
% to alter or adjust the chapter heading in the running head

\section{定义}
自然语言处理（Natural Language Processing，简称NLP）是用计算机来处理、理解以及运用人类语言，它属于人工智能的一个分支，是计算机科学与语言学的交叉学科，又常被称为计算语言学。由于自然语言是人类区别于其他动物的根本标志；没有语言，人类的思维也就无从谈起。所以自然语言处理体现了人工智能的最高任务与境界；也就是说，只有当计算机具备了处理自然语言的能力时，机器才算实现了真正的智能。

从研究内容来看，自然语言处理包括语法分析、语义分析、篇章理解等。从应用角度来看，自然语言处理具有广泛的应用前景。特别是在信息时代，自然语言处理的应用包罗万象，例如：机器翻译、手写体和印刷体字符识别、语音识别及语音合成、信息检索、信息抽取与过滤、文本分类与聚类、舆情分析和观点挖掘等，它涉及与语言处理相关的数据挖掘、机器学习、知识获取、知识工程、人工智能研究和与语言计算相关的语言学研究等。

自然语言处理研究的问题一般会涉及自然语言的形态学、语法学、语义学和语用学等几个层次。

形态学（morphology）：形态学是语言学的一个分支，重点研究词的内部结构，包括屈折变化和构词法两个部分。

语法学（syntax）：研究句子结构成分之间的相互关系和组成句子序列的规则。其关注的中心是：为什么一句话可以这么说，也可以那么说？

语义学（semantics）：语义学的研究对象是语言的各级单位（词素、词、词组、句子、句子群、段落、篇章）的意义，以及语义与语音、语法、语境的关系等等。其重点在探明符号与符号所指对象之间的关系，从而知道人们的语言活动。简而言之，它关注的重点是：这个语言单位到底说了什么？

语用学（pragmatics）：大概来说它必须说明的问题是多方面的，包括直指、会话隐含、预设、言语行为、话语结构等。它研究在不同上下文下的语句应用，以及上下文对语句理解所产生的影响。其关注重点在于：为什么在特定的上下文要说这句话？

自然语言处理需要解决的关键是歧义消解问题和未知语言现象的处理问题。一方面，自然语言中存在大量的歧义现象，无论是在词法层次、句法层次，还是在语义层次和语用层次，无论哪类语言单位，其歧义性始终是困扰人们实现应用目标的一个根本问题。

\section{发展历史}
对于自然语言处理的发展历程，可以从哲学中的理性主义和经验主义说起。基于规则的自然语言处理是哲学中的理性主义；基于统计的自然语言处理是哲学中的经验主义。理性主义方法认为，人类语言主要是由语言规则来产生和描述的，因此只要能够用适当的形式将人类语言规则表示出来，就能够理解人类语言，并实现语言之间的翻译等各种自然语言处理任务。而经验主义方法则认为，从语言数据中获取语言统计知识，有效建立语言的统计模型。因此只要能够有足够多的用于统计的语言数据，就能够理解人类语言。

早期的自然语言处理具有鲜明的经验主义色彩。如1913年马尔科夫提出马尔科夫随机过程与马尔科夫模型\cite{markov1913example}的基础就是手工计算长诗中元音与辅音出现的频度；1948年香农把离散马尔科夫的概率模型\cite{shannon1948mathematical}应用于语言的自动机，同时采用手工方法统计英语字母的频率。

然而这种经验主义到了乔姆斯基时出现了转变。1956年乔姆斯基\cite{chomsky1956three}借鉴香农的工作，把有限状态机用作刻画语法的工具，建立了自然语言的有限状态模型，具体来说就是用代数和集合论将语言转化为符号序列，找到了一种一种统一的数学理论来刻画自然语言，从此诞生了一个叫做“形式语言理论”的新领域。

在20世纪50年代末到60年代中期，经验主义东山再起了。多数学者普遍认为只有详尽的历史语料才能带来可靠的结论。于是一些比较著名的理论与算法就诞生了，如贝叶斯方法、隐马尔可夫\cite{stratonovich1965conditional}、最大熵\cite{jaynes1957information}、维特比算法\cite{viterbi1967error}、支持向量机\cite{}等之类。世界上第一个联机语料库Brown Corpus\cite{kuvcera1967computational}也是在那个时候的Brown University诞生的。但是总的来说，这个时代依然是基于规则的理性主义的天下，经验主义虽然取得了不俗的成就，却依然没有受到太大的重视。

随着90年代以来，基于统计的自然语言处理开始大放异彩了。首先是在机器翻译领域取得了突破，因为引入了许多基于语料库的方法。1990年在芬兰赫尔辛基举办的第13届国际计算语言学会议之后，大家的重心开始转向大规模真实文本了，传统的仅仅基于规则的自然语言处理显然力不从心了。学者们认为，大规模语料至少是对基于规则方法有效的补充。到了1994至1999年，经验主义就开始空前繁荣了。如句法剖析、词类标注、指代消解等算法几乎把概率与数据作为标准方法，成为了自然语言处理的主流。下面列举一些比较重要的里程碑。

1998年和接下来的几年里，FrameNet\cite{baker1998berkeley}项目指导了语义角色标注的任务。这是一种浅层语义解析的形式，后续促进了，如组块分析、命名实体识别、依存分析等核心NLP任务的研究。

2001年，条件随机场\cite{lafferty2001conditional}（Conditional Random Fields，CRF）成为了最具影响力的序列标注方法类别之一，获得了ICML 2011的最佳论文奖。CRF是当前最先进的序列标注问题模型的核心部分，这些模型具有标签间的相互依赖性，广泛用于分词、词性标注、专名识别等序列标注任务中。

2002年，双语互译质量评估辅助工具BLEU\cite{papineni2002bleu}，给出了双语互译质量度量标准，这使得机器翻译系统得以扩展，其现在仍然是机器翻译评估的标准度量标准。同年，结构化感知机\cite{collins2002discriminative}问世，为结构化感知工作奠定了基础。同时，情感分析\cite{pang2002thumbs}也成了最受欢迎和广泛研究的NLP任务之一。

在2003年，引入了潜在狄利克雷分布\cite{blei2003latent}，至今仍是主题建模的标准方法。在2004年，有学者提出了比SVM更适合于捕获结构化数据中的相关性的新最大边缘模型\cite{taskar2004max}。

2006年，发布了OntoNotes\cite{hovy2006ontonotes}这个大型多语言语料库，它已被用于训练和评估各种任务，如依赖解析和引用解析。在2008年，Milne和Witten\cite{witten2008effective}介绍了利用维基百科丰富机器学习方法的方案；到目前为止，维基百科仍然是最有用的资源之一，无论是用于实体链接和消除歧义、语言建模、知识库还是其他各种任务。

2009年，提出了远程监督\cite{go2009twitter}的概念。远程监督利用启发式或现有知识库中的信息生成带有噪声的模式，可用于从大型语料库中自动提取示例。远程监督现已被广泛应用，并且已经是关系提取、信息提取、情感分析等领域的常用技术。

特别指出近十几年来，随着大数据、神经网络、深度学习的快速发展，自然语言处理技术也大幅演化。下面也列举一些重要的里程碑。

2003年，Bengio等人提出基于神经网络的语言模型\cite{bengio2003neural}，用于替代传统基于计数和平滑的语言模型。语言建模是一种非监督学习形式，其被作为获得基础常识的先决条件。尽管语言建模很简单，但却是后续许多技术发展的核心。

2008年，Collobert和Weston将多任务学习\cite{collobert2008unified}首次应用于NLP的神经网络。在他们的模型中，查询表（或单词嵌入矩阵）在两个接受不同任务训练的模型之间共享。多任务学习现在被广泛地用于NLP任务。充分利用现有的或人造的任务进行训练，可以更好的提高NLP效率。

2013年，Mikolov等人提出的word2vec\cite{mikolov2013efficient}，使语言模型的词嵌入训练极大加速，也使得大规模的词嵌入训练成为可能。

2013年和2014年是开始引入神经网络模型到NLP任务的时期。使用最广泛的三种主要的神经网络是：卷积神经网络\cite{kim2014convolutional}\cite{kalchbrenner2014convolutional}、循环神经网络\cite{graves2013hybrid}和递归神经网络\cite{socher2013recursive}。

2014年，Sutskever等人提出了序列到序列模型\cite{sutskever2014sequence}。这是一个使用神经网络将一个序列映射到另一个序列的通用框架。在该框架中，编码器神经网络逐符号处理一个句子，并将其压缩为一个向量表示；然后，一个解码器神经网络根据编码器状态逐符号输出预测值，并将之前预测的符号作为每一步的输入。机器翻译是对这个框架比较成功的应用。

2014年，Bahdanau等人提出注意力机制\cite{bahdanau2014neural}，是神经网络机器翻译的核心创新之一，也是使神经网络机器翻译模型胜过经典的基于短语的机器翻译系统的关键思想。序列到序列模型的主要瓶颈是需要将源序列的全部内容压缩为一个固定大小的向量。注意力机制通过允许解码器回头查看源序列隐藏状态来缓解这一问题，然后将其加权平均作为额外输入提供给解码器。

大约同时在2015年前后，很多学者提出了基于记忆的网络。注意力机制可以看作是模糊记忆的一种形式。记忆由模型的隐藏状态组成，模型选择从记忆中检索内容。研究者们提出了许多具有更明确记忆的模型。这些模型有不同的变体，如神经图灵机\cite{graves2014neural}（Graves等，2014）、记忆网络\cite{weston2014memory}（Weston等，2015）和端到端记忆网络\cite{sukhbaatar2015end}（Sukhbaatar等，2015）、动态记忆网络\cite{kumar2016ask}（Kumar等，2015）、神经微分计算机\cite{graves2016hybrid}（Graves等，2016）和循环实体网络\cite{henaff2016tracking}（Henaff等，2017）。

2018年，Peters等证明利用预训练语言模型\cite{conneau2017supervised}\cite{mccann2017learned}\cite{subramanian2018learning}，各种NLP任务都能获得大幅度提升改进。通过将语言模型嵌入作为特征，使用目标任务数对语言模型对进行微调\cite{ramachandran2016unsupervised}\cite{howard2018universal}，通常就能达到或超过传统结果。由于语言模型只需要无标记的数据便可以进行学习，因此对于标记数据稀缺的低资源场景，预训练语言模型尤其有用。

\section{应用领域}
自然语言处理研究的内容非常广泛，应用范围也非常广泛，如下举例一些常见的应用场景：

\begin{itemize}
\item 机器翻译：利用计算机实现自然语言（英语、汉语等）之间的自动翻译。
\item 自动摘要：利用计算机自动地从原始文档中提取全面准确地反映该文档中心内容的简单连贯的短文。
\item 文本分类：在预定义分类体系下，根据文本的特征，将给定文本于一个或多个类别相关联的过程。
\item 情感分类：根据文本所表达的含义和情感将文本划分为褒扬或者贬义的两种或几种类型，是对作者倾向性、观点、态度的划分，因此也称倾向性分析。
\item 信息抽取：从非结构化或半结构的自然语言文本中提取出于某个主题相关的实体、关系。事件等事实信息，并且形成结构化信息输出。
\item 信息检索：用户输入一个表述需求信息的查询字段，系统回复一个包含所需要信息的文档列表。其核心技术在于索引构建和相关性计算。
\item 问答系统：接受用户自然语言形式描述的问题，从大量异构数据中查找或者推断用户问题答案的信息检索系统。
\end{itemize}
